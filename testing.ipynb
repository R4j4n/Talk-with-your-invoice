{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from donut import JSONParseEvaluator\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from config import model_name, model_name_base\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = DonutProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "f1 = []\n",
    "accs = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "dataset = load_dataset(\"Rajan/AIMT-invoices-donut-data\", split=\"test\")\n",
    "\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "    true_label = ground_truth[\"gt_parse\"]\n",
    "\n",
    "\n",
    "    ground_truth = ground_truth[\"gt_parse\"]\n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "    f1_value = evaluator.cal_f1(seq,ground_truth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for pred, answer in zip(seq, ground_truth):\n",
    "        pred, answer = evaluator.flatten(evaluator.normalize_dict(pred)), evaluator.flatten(evaluator.normalize_dict(answer))\n",
    "        answer_set = set(answer)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        tp = len(pred_set & answer_set)\n",
    "        fp = len(pred_set - answer_set)\n",
    "        fn = len(answer_set - pred_set)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "\n",
    "\n",
    "    accs.append(score)\n",
    "    f1.append(f1_value)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "\n",
    "    \n",
    "scores_our = {\"mean_f1\": np.mean(f1), \"mean_accuracy\": np.mean(accs), \"recall\": np.mean(recalls), \"precisions\": np.mean(precisions)}\n",
    "# scores_our.update({n: 3 * scores_our[n] for n in scores_our.keys()})\n",
    "scores_our.update({n: 100 * scores_our[n] for n in scores_our.keys()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = DonutProcessor.from_pretrained(model_name_base)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "f1 = []\n",
    "accs = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "    true_label = ground_truth[\"gt_parse\"]\n",
    "\n",
    "\n",
    "    ground_truth = ground_truth[\"gt_parse\"]\n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "    f1_value = evaluator.cal_f1(seq,ground_truth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for pred, answer in zip(seq, ground_truth):\n",
    "        pred, answer = evaluator.flatten(evaluator.normalize_dict(pred)), evaluator.flatten(evaluator.normalize_dict(answer))\n",
    "        answer_set = set(answer)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        tp = len(pred_set & answer_set)\n",
    "        fp = len(pred_set - answer_set)\n",
    "        fn = len(answer_set - pred_set)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "\n",
    "\n",
    "    accs.append(score)\n",
    "    f1.append(f1_value)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "\n",
    "    \n",
    "scores_others = {\"mean_f1\": np.mean(f1), \"mean_accuracy\": np.mean(accs), \"recall\": np.mean(recalls), \"precisions\": np.mean(precisions)}\n",
    "\n",
    "scores_others.update({n: 100 * scores_others[n] for n in scores_others.keys()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comparison_data = {\n",
    "    \"Metric\": [\"mean_f1\", \"mean_accuracy\", \"recall\", \"precisions\"],\n",
    "    \"Scores_Others\": [scores_others[\"mean_f1\"], scores_others[\"mean_accuracy\"], scores_others[\"recall\"], scores_others[\"precisions\"]],\n",
    "    \"Scores_Our\": [scores_our[\"mean_f1\"], scores_our[\"mean_accuracy\"], scores_our[\"recall\"], scores_our[\"precisions\"]]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_term2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
