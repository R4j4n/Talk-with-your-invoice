{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######## TESTING ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjn/miniconda3/envs/ttb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from donut import JSONParseEvaluator\n",
    "from datasets import load_dataset\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import model_name_10,model_name_30,model_name_base\n",
    "# model_name_10 : model trained for 10 epochs\n",
    "# model_name_30 : model trained for 30 epochs\n",
    "# model_name_base : Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]/home/rjn/miniconda3/envs/ttb/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 26/26 [01:53<00:00,  4.36s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(model_name_10)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name_10)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "f1 = []\n",
    "accs = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "evaluator = JSONParseEvaluator()\n",
    "dataset = load_dataset(\"Rajan/AIMT-invoices-donut-data\", split=\"test\")\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # Prepare encoder inputs\n",
    "    pixel_values = processor(\n",
    "        sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\"\n",
    "    ).pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # Prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # Autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # Turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(\n",
    "        processor.tokenizer.pad_token, \"\"\n",
    "    )\n",
    "    seq = re.sub(\n",
    "        r\"<.*?>\", \"\", seq, count=1\n",
    "    ).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])[\"gt_parse\"]\n",
    "\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "    f1_value = evaluator.cal_f1(seq, ground_truth)\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for pred, answer in zip(seq, ground_truth):\n",
    "        pred, answer = evaluator.flatten(\n",
    "            evaluator.normalize_dict(pred)\n",
    "        ), evaluator.flatten(evaluator.normalize_dict(answer))\n",
    "        answer_set = set(answer)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        tp = len(pred_set & answer_set)\n",
    "        fp = len(pred_set - answer_set)\n",
    "        fn = len(answer_set - pred_set)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    accs.append(score)\n",
    "    f1.append(f1_value)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "\n",
    "scores_10_epoch = {\n",
    "    \"mean_f1\": np.mean(f1),\n",
    "    \"mean_accuracy\": np.mean(accs),\n",
    "    \"recall\": np.mean(recalls),\n",
    "    \"precisions\": np.mean(precisions),\n",
    "}\n",
    "# scores_10_epoch.update({n: 100 * scores_10_epoch[n] for n in scores_10_epoch.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear cache for other model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:53<00:00,  4.38s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(model_name_base)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name_base)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "f1 = []\n",
    "accs = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "evaluator = JSONParseEvaluator()\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # Prepare encoder inputs\n",
    "    pixel_values = processor(\n",
    "        sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\"\n",
    "    ).pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # Prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # Autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # Turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(\n",
    "        processor.tokenizer.pad_token, \"\"\n",
    "    )\n",
    "    seq = re.sub(\n",
    "        r\"<.*?>\", \"\", seq, count=1\n",
    "    ).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])[\"gt_parse\"]\n",
    "\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "    f1_value = evaluator.cal_f1(seq, ground_truth)\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for pred, answer in zip(seq, ground_truth):\n",
    "        pred, answer = evaluator.flatten(\n",
    "            evaluator.normalize_dict(pred)\n",
    "        ), evaluator.flatten(evaluator.normalize_dict(answer))\n",
    "        answer_set = set(answer)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        tp = len(pred_set & answer_set)\n",
    "        fp = len(pred_set - answer_set)\n",
    "        fn = len(answer_set - pred_set)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    accs.append(score)\n",
    "    f1.append(f1_value)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "\n",
    "scores_base = {\n",
    "    \"mean_f1\": np.mean(f1),\n",
    "    \"mean_accuracy\": np.mean(accs),\n",
    "    \"recall\": np.mean(recalls),\n",
    "    \"precisions\": np.mean(precisions),\n",
    "}\n",
    "# scores_base.update({n: 100 * scores_base[n] for n in scores_base.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear cache for other model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 17/17 [01:16<00:00,  4.53s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(model_name_30)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name_30)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "f1 = []\n",
    "accs = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "evaluator = JSONParseEvaluator()\n",
    "dataset = load_dataset(\"Rajan/AIMT-invoices-donut-data\", split=\"test\")\n",
    "dataset = dataset.select(range(17))\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # Prepare encoder inputs\n",
    "    pixel_values = processor(\n",
    "        sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\"\n",
    "    ).pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # Prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # Autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # Turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(\n",
    "        processor.tokenizer.pad_token, \"\"\n",
    "    )\n",
    "    seq = re.sub(\n",
    "        r\"<.*?>\", \"\", seq, count=1\n",
    "    ).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])[\"gt_parse\"]\n",
    "\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "    f1_value = evaluator.cal_f1(seq, ground_truth)\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for pred, answer in zip(seq, ground_truth):\n",
    "        pred, answer = evaluator.flatten(\n",
    "            evaluator.normalize_dict(pred)\n",
    "        ), evaluator.flatten(evaluator.normalize_dict(answer))\n",
    "        answer_set = set(answer)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        tp = len(pred_set & answer_set)\n",
    "        fp = len(pred_set - answer_set)\n",
    "        fn = len(answer_set - pred_set)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "\n",
    "    accs.append(score)\n",
    "    f1.append(f1_value)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "\n",
    "scores_30epochs = {\n",
    "    \"mean_f1\": np.mean(f1),\n",
    "    \"mean_accuracy\": np.mean(accs),\n",
    "    \"recall\": np.mean(recalls),\n",
    "    \"precisions\": np.mean(precisions),\n",
    "}\n",
    "# scores_30epochs.update({n: 100 * scores_base[n] for n in scores_base.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comparison_data = {\n",
    "    \"Metric\": [\"mean_f1\", \"mean_accuracy\", \"mean_recall\", \"mean_precision\"],\n",
    "    \"score_10_epochs\": [scores_10_epoch[\"mean_f1\"], scores_10_epoch[\"mean_accuracy\"], scores_10_epoch[\"recall\"], scores_10_epoch[\"precisions\"]],\n",
    "    \"score_30_epochs\": [scores_30epochs[\"mean_f1\"], scores_30epochs[\"mean_accuracy\"], scores_30epochs[\"recall\"], scores_30epochs[\"precisions\"]],\n",
    "    \"Score_Base\": [scores_base[\"mean_f1\"], scores_base[\"mean_accuracy\"], scores_base[\"recall\"], scores_base[\"precisions\"]],\n",
    "\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>score_10_epochs</th>\n",
       "      <th>score_30_epochs</th>\n",
       "      <th>Score_Base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean_f1</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean_accuracy</td>\n",
       "      <td>0.327561</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.949793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean_recall</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean_precision</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Metric  score_10_epochs  score_30_epochs  Score_Base\n",
       "0         mean_f1         0.230769         0.941176    0.961538\n",
       "1   mean_accuracy         0.327561         0.927053    0.949793\n",
       "2     mean_recall         0.230769         0.941176    0.961538\n",
       "3  mean_precision         0.230769         0.941176    0.961538"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_term2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
